{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple implementation of our NRMS model\n",
    "\n",
    "## Forord\n",
    "\n",
    "Denne notebook er et forsøg på at lave en sammenhængende og forståelig\n",
    "gennemgang af vores projekt fra A og næsten helt til Z.\n",
    "\n",
    "1) Initialization\n",
    "2) Data preprocessing\n",
    "3) Dataset class and dataloader\n",
    "4) Simple implementation of NRMS\n",
    "5) Basic training loop\n",
    "6) Metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "- Make separate file with all constants defined and columns\n",
    "- Address duplicate values in the behaviors data frame (for now removed them)\n",
    "- Find out if we should reshape, or squeeze, the 5 inview samples in our batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adamax/.pyenv/versions/anaconda3-2024.06-1/envs/DLProject_recsys-ebnerd/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "## IMPORTS\n",
    "\n",
    "# System imports\n",
    "import sys\n",
    "import os\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "# RecSys / eb-nerd imports\n",
    "from ebrec.utils._behaviors import (create_binary_labels_column, truncate_history, sampling_strategy_wu2019)\n",
    "from ebrec.utils._articles import create_article_id_to_value_mapping\n",
    "from ebrec.utils._articles_behaviors import map_list_article_id_to_value\n",
    "#from ebrec.models.newsrec.dataloader import NRMSDataLoader\n",
    "#from ebrec.evaluation import MetricEvaluator, AucScore, NdcgScore, MrrScore\n",
    "\n",
    "# Standard data science imports\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "from ebrec.utils._articles_behaviors import map_list_article_id_to_value\n",
    "from ebrec.utils._python import (\n",
    "    repeat_by_list_values_from_matrix,\n",
    "    create_lookup_objects,\n",
    ")\n",
    "\n",
    "from myDataloader import NewsrecDataset, create_dataloader\n",
    "\n",
    "################################################################################\n",
    "## VARIABLES\n",
    "################################################################################\n",
    "\n",
    "HISTORY_SIZE = 30   # This is to control how many clicked articles are kept in history\n",
    "NPRATIO = 4         # This is the same as K in the paper\n",
    "SEED = 123          # Seed for reproducibility\n",
    "BATCH_SIZE = 32     # Batch size given as input to the DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Data preprocessing\n",
    "\n",
    "Before we begin preprocessing the data to make the joined data frame for training,\n",
    "we very briefly explore the data as to get a broad overview of columns etc.\n",
    "\n",
    "Then we start preprocessing the data and finally join together the impression \n",
    "logs (behaviours) and users' browsed articles (history)\n",
    "\n",
    "### 2.1 Load data\n",
    "\n",
    "We use scan and then collect the data later while we process. Then we print the \n",
    "columns of each of the training data frames as well as the articles data frame\n",
    "because we want to organize the columns afterwards in the following way:\n",
    "1. Key columns\n",
    "2. Essential columns (denoted USE_COLUMNS below)\n",
    "3. Detail columns (the rest of the columns that are not essential, but would \n",
    "    probably improve the model if we implement them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to scan data from \n",
      "\t/Users/adamax/Documents/Studie/main/kurser/08_Fall_2024/02456_Deep_Learning/final_project/DeepLearning02456/tmp/Data/ebnerd_small\n",
      "Data scanned successfully...\n",
      "\n",
      "Columns for Behaviors Train:\n",
      "-----------------------------\n",
      "impression_id          : UInt32\n",
      "article_id             : Int32\n",
      "impression_time        : Datetime(time_unit='us', time_zone=None)\n",
      "read_time              : Float32\n",
      "scroll_percentage      : Float32\n",
      "device_type            : Int8\n",
      "article_ids_inview     : List(Int32)\n",
      "article_ids_clicked    : List(Int32)\n",
      "user_id                : UInt32\n",
      "is_sso_user            : Boolean\n",
      "gender                 : Int8\n",
      "postcode               : Int8\n",
      "age                    : Int8\n",
      "is_subscriber          : Boolean\n",
      "session_id             : UInt32\n",
      "next_read_time         : Float32\n",
      "next_scroll_percentage : Float32\n",
      "\n",
      "Columns for History Train:\n",
      "---------------------------\n",
      "user_id                 : UInt32\n",
      "impression_time_fixed   : List(Datetime(time_unit='us', time_zone=None))\n",
      "scroll_percentage_fixed : List(Float32)\n",
      "article_id_fixed        : List(Int32)\n",
      "read_time_fixed         : List(Float32)\n",
      "\n",
      "Columns for Articles:\n",
      "----------------------\n",
      "article_id         : Int32\n",
      "title              : String\n",
      "subtitle           : String\n",
      "last_modified_time : Datetime(time_unit='us', time_zone=None)\n",
      "premium            : Boolean\n",
      "body               : String\n",
      "published_time     : Datetime(time_unit='us', time_zone=None)\n",
      "image_ids          : List(Int64)\n",
      "article_type       : String\n",
      "url                : String\n",
      "ner_clusters       : List(String)\n",
      "entity_groups      : List(String)\n",
      "topics             : List(String)\n",
      "category           : Int16\n",
      "subcategory        : List(Int16)\n",
      "category_str       : String\n",
      "total_inviews      : Int32\n",
      "total_pageviews    : Int32\n",
      "total_read_time    : Float32\n",
      "sentiment_score    : Float32\n",
      "sentiment_label    : String\n",
      "contrastive_vector : List(Float32)\n",
      "\n",
      "Columns for Article embeddings:\n",
      "--------------------------------\n",
      "article_id         : Int32\n",
      "contrastive_vector : List(Float32)\n",
      "\n",
      "All schemas printed successfully.\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "## SCAN / LOAD DATA\n",
    "################################################################################\n",
    "\n",
    "# Insert your own data path\n",
    "DATA_PATH = Path(\"~/Documents/Studie/main/kurser/08_Fall_2024/02456_Deep_Learning/final_project/DeepLearning02456/tmp/Data\").expanduser()\n",
    "DATA_SIZE = f\"ebnerd_small\"     # [ebnerd_demo, ebnerd_small, ebnerd_large]\n",
    "EMBEDDING_PATH = f\"Ekstra_Bladet_contrastive_vector\"\n",
    "\n",
    "print(f\"Attempting to scan data from \\n\\t{DATA_PATH / DATA_SIZE}\")\n",
    "\n",
    "# Scan data (this means that the data is not loaded into memory and we have to collect it when needed)\n",
    "df_behaviors_train = pl.scan_parquet(\n",
    "    DATA_PATH.joinpath(DATA_SIZE, \"train\", \"behaviors.parquet\")\n",
    ")\n",
    "df_history_train = pl.scan_parquet(\n",
    "    DATA_PATH.joinpath(DATA_SIZE, \"train\", \"history.parquet\")\n",
    ")\n",
    "df_behaviors_val = df_behaviors = pl.scan_parquet(\n",
    "    DATA_PATH.joinpath(DATA_SIZE, \"validation\", \"behaviors.parquet\")\n",
    ")\n",
    "df_history_val = df_behaviors = pl.scan_parquet(\n",
    "    DATA_PATH.joinpath(DATA_SIZE, \"validation\", \"history.parquet\")\n",
    ")\n",
    "df_articles = pl.scan_parquet(DATA_PATH.joinpath(\"articles.parquet\"))\n",
    "\n",
    "# Load the contrastive vectors\n",
    "article_embeddings = pl.scan_parquet(DATA_PATH.joinpath(EMBEDDING_PATH, \"contrastive_vector.parquet\"))\n",
    "\n",
    "# Add the embeddings as a new column to the articles data frame and collect it\n",
    "df_articles = df_articles.join(article_embeddings, on=\"article_id\", how=\"left\").collect()\n",
    "\n",
    "print(f\"Data scanned successfully...\")\n",
    "\n",
    "## Doing some quick exploration\n",
    "# Keep data frames in a dictionary for easy access in the loop\n",
    "scanned_dataframes = {\n",
    "    \"Behaviors Train\": df_behaviors_train,\n",
    "    \"History Train\": df_history_train,\n",
    "    \"Articles\": df_articles,\n",
    "    \"Article embeddings\": article_embeddings,\n",
    "}\n",
    "\n",
    "# Loop through each scanned data frame\n",
    "for name, df in scanned_dataframes.items():\n",
    "    print(f\"\\nColumns for {name}:\")\n",
    "    print(\"-\" * (len(name) + 14)) # for proper alignment\n",
    "\n",
    "    schema = df.schema\n",
    "    max_col_len = max(len(col) for col in schema.keys())\n",
    "\n",
    "    # Print columns and types with alignment\n",
    "    for col, dtype in schema.items():\n",
    "        print(f\"{col:<{max_col_len}} : {dtype}\")\n",
    "\n",
    "print(\"\\nAll schemas printed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was the very quick overview of the data. \n",
    "\n",
    "### 2.2 Preprocess data\n",
    "Now we want to make the setup for joining the data frames together into one\n",
    "data frame for training.\n",
    "\n",
    "1. First we organize the columns from each data frame based on whether they are essential\n",
    "to make our model work:\n",
    "- Key columns\n",
    "- Use columns (essential columns)\n",
    "- Detail columns (other columns) \n",
    "2. Then we do some processing of the columns in the data frames to clean up\n",
    "the data a bit, before joining them.\n",
    "3. We join the two data frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 - Organizing columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columns to be used from History: ['user_id', 'article_id_fixed']\n",
      "Columns to be used from Behaviors: ['user_id', 'impression_id', 'impression_time', 'article_ids_clicked', 'article_ids_inview']\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "## SETUP FOR CONSTRUCTING OUR TRAINING DATA FRAME (USING POLARS)\n",
    "################################################################################\n",
    "\n",
    "## (1) Organizing the columns\n",
    "\n",
    "HISTORY_PREFIX = \"his\"\n",
    "IMPRESSION_PREFIX = \"imp\"\n",
    "ARTICLE_PREFIX = \"art\"\n",
    "\n",
    "# Organizing the columns in the data frames based on their importance to make it work\n",
    "IMPRESSION_KEY_COLUMNS = [\n",
    "    \"user_id\",                  # UInt32\n",
    "    \"impression_id\"\n",
    "]\n",
    "\n",
    "IMPRESSION_USE_COLUMNS = [\n",
    "    \"impression_time\",          # Datetime(time_unit='us', time_zone=None)\n",
    "    \"article_ids_clicked\",      # List(UInt32)\n",
    "    \"article_ids_inview\"        # List(UInt32)\n",
    "]\n",
    "\n",
    "IMPRESSION_DETAIL_COLUMNS = [\n",
    "    \"read_time\",              # Float32\n",
    "    \"scroll_percentage\",      # Float32\n",
    "    \"device_type\",            # Int8\n",
    "    \"is_sso_user\",            # Boolean\n",
    "    \"gender\",                 # Int8\n",
    "    \"postcode\",               # Int8\n",
    "    \"age\",                    # Int8\n",
    "    \"is_subscriber\",          # Boolean\n",
    "    \"session_id\",             # UInt32\n",
    "    \"next_read_time\",         # Float32\n",
    "    \"next_scroll_percentage\"  # Float32\n",
    "]\n",
    "\n",
    "HISTORY_KEY_COLUMNS = [\n",
    "    \"user_id\",                # UInt32\n",
    "]\n",
    "\n",
    "HISTORY_USE_COLUMNS = [\n",
    "    \"article_id_fixed\"        # List(UInt32)\n",
    "]\n",
    "\n",
    "HISTORY_DETAIL_COLUMNS = [\n",
    "    \"scroll_percentage_fixed\" # List(Float32)\n",
    "    \"article_id_fixed\"        # List(Int32)\n",
    "    \"read_time_fixed\"         # List(Float32)\n",
    "]\n",
    "\n",
    "# OBS! Articles not used for now\n",
    "# TODO - Add article columns\n",
    "ARTICLE_KEY_COLUMNS = [\n",
    "    \"article_id\"  # Int32\n",
    "]\n",
    "\n",
    "ARTICLE_USE_COLUMNS = [\n",
    "    \"title\",              # String\n",
    "    \"subtitle\",           # String\n",
    "    \"published_time\",     # Datetime(time_unit='us', time_zone=None)\n",
    "    \"category\",           # Int16\n",
    "    \"subcategory\",        # List(Int16)\n",
    "    \"category_str\",       # String\n",
    "    \"sentiment_score\",    # Float32\n",
    "    \"sentiment_label\"     # String\n",
    "]\n",
    "\n",
    "ARTICLE_DETAIL_COLUMNS = [\n",
    "    \"body\",               # String\n",
    "    \"last_modified_time\", # Datetime(time_unit='us', time_zone=None)\n",
    "    \"premium\",            # Boolean\n",
    "    \"image_ids\",          # List(Int64)\n",
    "    \"article_type\",       # String\n",
    "    \"url\",                # String\n",
    "    \"ner_clusters\",       # List(String)\n",
    "    \"entity_groups\",      # List(String)\n",
    "    \"topics\",             # List(String)\n",
    "    \"total_inviews\",      # Int32\n",
    "    \"total_pageviews\",    # Int32\n",
    "    \"total_read_time\",    # Float32\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "## Here I gather the columns I want to use for my training data frame\n",
    "COLUMNS_FROM_HISTORY = HISTORY_KEY_COLUMNS + HISTORY_USE_COLUMNS\n",
    "COLUMNS_FROM_BEHAVIORS = IMPRESSION_KEY_COLUMNS + IMPRESSION_USE_COLUMNS\n",
    "\n",
    "print(f\"\\nColumns to be used from History: {COLUMNS_FROM_HISTORY}\")\n",
    "print(f\"Columns to be used from Behaviors: {COLUMNS_FROM_BEHAVIORS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 - Preprocessing\n",
    "\n",
    "**Choices for history data frame**\n",
    "- Selecting only essential columns \n",
    "- Renaming article_id_fixed to his_article_ids\n",
    "- Truncating number of articles in history\n",
    "- TODO: apply exponential decay from utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of df_history_train after processing: (15143, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>his_article_ids</th></tr><tr><td>u32</td><td>list[i32]</td></tr></thead><tbody><tr><td>13538</td><td>[9767342, 9767751, … 9769366]</td></tr><tr><td>14241</td><td>[9763401, 9763250, … 9767852]</td></tr><tr><td>20396</td><td>[9763634, 9763401, … 9769679]</td></tr><tr><td>34912</td><td>[9766722, 9759476, … 9770882]</td></tr><tr><td>37953</td><td>[9762836, 9763942, … 9769306]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 2)\n",
       "┌─────────┬───────────────────────────────┐\n",
       "│ user_id ┆ his_article_ids               │\n",
       "│ ---     ┆ ---                           │\n",
       "│ u32     ┆ list[i32]                     │\n",
       "╞═════════╪═══════════════════════════════╡\n",
       "│ 13538   ┆ [9767342, 9767751, … 9769366] │\n",
       "│ 14241   ┆ [9763401, 9763250, … 9767852] │\n",
       "│ 20396   ┆ [9763634, 9763401, … 9769679] │\n",
       "│ 34912   ┆ [9766722, 9759476, … 9770882] │\n",
       "│ 37953   ┆ [9762836, 9763942, … 9769306] │\n",
       "└─────────┴───────────────────────────────┘"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (2) Now we start the actual processing of the data frames\n",
    "\n",
    "## HISTORY DATA FRAME\n",
    "# Process the history data frame by selecting, renaming and then truncating the history\n",
    "df_history_train = (\n",
    "    df_history_train\n",
    "    .select(COLUMNS_FROM_HISTORY)                                  # Selecting the columns we want to keep\n",
    "    .rename({\"article_id_fixed\": f\"{HISTORY_PREFIX}_article_ids\"}) # using prefix: his_ to indicate origin: history\n",
    "    #.with_columns(pl.col(\"his_article_ids\").list.len().alias(\"his_num_articles\")) # This line adds another column with the number of clicked articles in the history prior to truncation\n",
    "    .pipe(                                                         # Truncating the history\n",
    "            truncate_history,\n",
    "            column=\"his_article_ids\",\n",
    "            history_size=HISTORY_SIZE,\n",
    "            padding_value=0,\n",
    "            enable_warning=False,\n",
    "        )\n",
    "    .collect()  # Collecting the data frame into memory in the end\n",
    "    )\n",
    "\n",
    "# Quick check that it worked\n",
    "print(f\"shape of df_history_train after processing: {df_history_train.shape}\")\n",
    "df_history_train.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choices for behaviors data frame**\n",
    "- Selecting essential columns\n",
    "- Filter out impressions with more than one clicked article\n",
    "- Negative sampling with postive:negative ratio 1:4 (I actually think it should be higher)\n",
    "- Make label column called click\n",
    "- Expand article_ids_inview and rename to article_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of df_behaviors_train after processing: (231731, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>impression_id</th><th>impression_time</th><th>article_ids_clicked</th><th>article_ids_inview</th><th>labels</th></tr><tr><td>u32</td><td>u32</td><td>datetime[μs]</td><td>list[i64]</td><td>list[i64]</td><td>list[i8]</td></tr></thead><tbody><tr><td>139836</td><td>149474</td><td>2023-05-24 07:47:53</td><td>[9778657]</td><td>[9778728, 9778669, … 9778657]</td><td>[0, 0, … 1]</td></tr><tr><td>143471</td><td>150528</td><td>2023-05-24 07:33:25</td><td>[9778623]</td><td>[9778669, 9778769, … 9778623]</td><td>[0, 0, … 1]</td></tr><tr><td>151570</td><td>153068</td><td>2023-05-24 07:09:04</td><td>[9778669]</td><td>[9772866, 9776259, … 9693002]</td><td>[0, 0, … 0]</td></tr><tr><td>151570</td><td>153070</td><td>2023-05-24 07:13:14</td><td>[9778628]</td><td>[9430567, 9778628, … 9525589]</td><td>[0, 1, … 0]</td></tr><tr><td>151570</td><td>153071</td><td>2023-05-24 07:11:08</td><td>[9777492]</td><td>[9131971, 9335113, … 9778623]</td><td>[0, 0, … 0]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 6)\n",
       "┌─────────┬───────────────┬───────────────────┬───────────────────┬──────────────────┬─────────────┐\n",
       "│ user_id ┆ impression_id ┆ impression_time   ┆ article_ids_click ┆ article_ids_invi ┆ labels      │\n",
       "│ ---     ┆ ---           ┆ ---               ┆ ed                ┆ ew               ┆ ---         │\n",
       "│ u32     ┆ u32           ┆ datetime[μs]      ┆ ---               ┆ ---              ┆ list[i8]    │\n",
       "│         ┆               ┆                   ┆ list[i64]         ┆ list[i64]        ┆             │\n",
       "╞═════════╪═══════════════╪═══════════════════╪═══════════════════╪══════════════════╪═════════════╡\n",
       "│ 139836  ┆ 149474        ┆ 2023-05-24        ┆ [9778657]         ┆ [9778728,        ┆ [0, 0, … 1] │\n",
       "│         ┆               ┆ 07:47:53          ┆                   ┆ 9778669, …       ┆             │\n",
       "│         ┆               ┆                   ┆                   ┆ 9778657]         ┆             │\n",
       "│ 143471  ┆ 150528        ┆ 2023-05-24        ┆ [9778623]         ┆ [9778669,        ┆ [0, 0, … 1] │\n",
       "│         ┆               ┆ 07:33:25          ┆                   ┆ 9778769, …       ┆             │\n",
       "│         ┆               ┆                   ┆                   ┆ 9778623]         ┆             │\n",
       "│ 151570  ┆ 153068        ┆ 2023-05-24        ┆ [9778669]         ┆ [9772866,        ┆ [0, 0, … 0] │\n",
       "│         ┆               ┆ 07:09:04          ┆                   ┆ 9776259, …       ┆             │\n",
       "│         ┆               ┆                   ┆                   ┆ 9693002]         ┆             │\n",
       "│ 151570  ┆ 153070        ┆ 2023-05-24        ┆ [9778628]         ┆ [9430567,        ┆ [0, 1, … 0] │\n",
       "│         ┆               ┆ 07:13:14          ┆                   ┆ 9778628, …       ┆             │\n",
       "│         ┆               ┆                   ┆                   ┆ 9525589]         ┆             │\n",
       "│ 151570  ┆ 153071        ┆ 2023-05-24        ┆ [9777492]         ┆ [9131971,        ┆ [0, 0, … 0] │\n",
       "│         ┆               ┆ 07:11:08          ┆                   ┆ 9335113, …       ┆             │\n",
       "│         ┆               ┆                   ┆                   ┆ 9778623]         ┆             │\n",
       "└─────────┴───────────────┴───────────────────┴───────────────────┴──────────────────┴─────────────┘"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## BEHAVIORS DATA FRAME\n",
    "df_behaviors_train = (\n",
    "  df_behaviors_train\n",
    "    .select(COLUMNS_FROM_BEHAVIORS) # selecting the columns we want to keep\n",
    "    .with_columns(\n",
    "        length=pl.col('article_ids_clicked').map_elements(lambda x: len(x)))  # adding a column with the length of the clicked articles\n",
    "    .filter(pl.col('length') == 1)  # we only want users with exactly one click in their impression\n",
    "    .collect()                      # Collecting the data frame into memory\n",
    "    .pipe(sampling_strategy_wu2019, npratio=NPRATIO, shuffle=True, clicked_col=\"article_ids_clicked\",\n",
    "          inview_col=\"article_ids_inview\", with_replacement=False, seed=SEED)   # down-sampling\n",
    "    .pipe(create_binary_labels_column, clicked_col=\"article_ids_clicked\",      # creating the binary labels column\n",
    "          inview_col=\"article_ids_inview\")\n",
    "    .drop(\"length\")\n",
    ")\n",
    "\n",
    "# WE WILL NOT USE THIS FOR NOW\n",
    "#     .rename({\"article_ids_inview\": \"article_id\"}) # renaming to article id because we expand this column now\n",
    "#     .explode(\"article_id\") # expanding the article ids in view\n",
    "#     .with_columns(click=pl.col(\"article_id\").is_in(pl.col(\"article_ids_clicked\")).cast(pl.Int8))\n",
    "#     .drop([\"article_ids_clicked\", \"length\", \"labels\"])\n",
    "#     .with_columns(pl.col(\"article_id\").cast(pl.Int32))\n",
    "\n",
    "print(f\"shape of df_behaviors_train after processing: {df_behaviors_train.shape}\")\n",
    "df_behaviors_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Joining the data  frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of joined train data frame: (231731, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 7)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>impression_id</th><th>impression_time</th><th>article_ids_clicked</th><th>article_ids_inview</th><th>labels</th><th>his_article_ids</th></tr><tr><td>u32</td><td>u32</td><td>datetime[μs]</td><td>list[i64]</td><td>list[i64]</td><td>list[i8]</td><td>list[i32]</td></tr></thead><tbody><tr><td>139836</td><td>149474</td><td>2023-05-24 07:47:53</td><td>[9778657]</td><td>[9778728, 9778669, … 9778657]</td><td>[0, 0, … 1]</td><td>[0, 0, … 9765156]</td></tr><tr><td>143471</td><td>150528</td><td>2023-05-24 07:33:25</td><td>[9778623]</td><td>[9778669, 9778769, … 9778623]</td><td>[0, 0, … 1]</td><td>[9767557, 9768062, … 9770989]</td></tr><tr><td>151570</td><td>153068</td><td>2023-05-24 07:09:04</td><td>[9778669]</td><td>[9772866, 9776259, … 9693002]</td><td>[0, 0, … 0]</td><td>[9770620, 9770594, … 9770829]</td></tr><tr><td>151570</td><td>153070</td><td>2023-05-24 07:13:14</td><td>[9778628]</td><td>[9430567, 9778628, … 9525589]</td><td>[0, 1, … 0]</td><td>[9770620, 9770594, … 9770829]</td></tr><tr><td>151570</td><td>153071</td><td>2023-05-24 07:11:08</td><td>[9777492]</td><td>[9131971, 9335113, … 9778623]</td><td>[0, 0, … 0]</td><td>[9770620, 9770594, … 9770829]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 7)\n",
       "┌─────────┬──────────────┬──────────────┬──────────────┬──────────────┬─────────────┬──────────────┐\n",
       "│ user_id ┆ impression_i ┆ impression_t ┆ article_ids_ ┆ article_ids_ ┆ labels      ┆ his_article_ │\n",
       "│ ---     ┆ d            ┆ ime          ┆ clicked      ┆ inview       ┆ ---         ┆ ids          │\n",
       "│ u32     ┆ ---          ┆ ---          ┆ ---          ┆ ---          ┆ list[i8]    ┆ ---          │\n",
       "│         ┆ u32          ┆ datetime[μs] ┆ list[i64]    ┆ list[i64]    ┆             ┆ list[i32]    │\n",
       "╞═════════╪══════════════╪══════════════╪══════════════╪══════════════╪═════════════╪══════════════╡\n",
       "│ 139836  ┆ 149474       ┆ 2023-05-24   ┆ [9778657]    ┆ [9778728,    ┆ [0, 0, … 1] ┆ [0, 0, …     │\n",
       "│         ┆              ┆ 07:47:53     ┆              ┆ 9778669, …   ┆             ┆ 9765156]     │\n",
       "│         ┆              ┆              ┆              ┆ 9778657]     ┆             ┆              │\n",
       "│ 143471  ┆ 150528       ┆ 2023-05-24   ┆ [9778623]    ┆ [9778669,    ┆ [0, 0, … 1] ┆ [9767557,    │\n",
       "│         ┆              ┆ 07:33:25     ┆              ┆ 9778769, …   ┆             ┆ 9768062, …   │\n",
       "│         ┆              ┆              ┆              ┆ 9778623]     ┆             ┆ 9770989]     │\n",
       "│ 151570  ┆ 153068       ┆ 2023-05-24   ┆ [9778669]    ┆ [9772866,    ┆ [0, 0, … 0] ┆ [9770620,    │\n",
       "│         ┆              ┆ 07:09:04     ┆              ┆ 9776259, …   ┆             ┆ 9770594, …   │\n",
       "│         ┆              ┆              ┆              ┆ 9693002]     ┆             ┆ 9770829]     │\n",
       "│ 151570  ┆ 153070       ┆ 2023-05-24   ┆ [9778628]    ┆ [9430567,    ┆ [0, 1, … 0] ┆ [9770620,    │\n",
       "│         ┆              ┆ 07:13:14     ┆              ┆ 9778628, …   ┆             ┆ 9770594, …   │\n",
       "│         ┆              ┆              ┆              ┆ 9525589]     ┆             ┆ 9770829]     │\n",
       "│ 151570  ┆ 153071       ┆ 2023-05-24   ┆ [9777492]    ┆ [9131971,    ┆ [0, 0, … 0] ┆ [9770620,    │\n",
       "│         ┆              ┆ 07:11:08     ┆              ┆ 9335113, …   ┆             ┆ 9770594, …   │\n",
       "│         ┆              ┆              ┆              ┆ 9778623]     ┆             ┆ 9770829]     │\n",
       "└─────────┴──────────────┴──────────────┴──────────────┴──────────────┴─────────────┴──────────────┘"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = (\n",
    "    df_behaviors_train\n",
    "    .join(df_history_train, on=\"user_id\", how=\"left\")\n",
    ")\n",
    "\n",
    "print(f\"shape of joined train data frame: {df_train.shape}\")\n",
    "df_train.head(5)  # Quick check that it worked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will save this data frame, so we don't have to process it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The preprocessed data frame for training is already saved as df_train_basic.parquet\n"
     ]
    }
   ],
   "source": [
    "# Saving df_train in parquet format and csv\n",
    "df_train_file_name = \"df_train_basic\"\n",
    "\n",
    "if (DATA_PATH / f\"{df_train_file_name}.parquet\").exists():\n",
    "    print(f\"The preprocessed data frame for training is already saved as {df_train_file_name}.parquet\")\n",
    "else:\n",
    "    print(\"File did not exist... saving now.\")\n",
    "    df_train.write_parquet(f\"{DATA_PATH}/{df_train_file_name}.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Dataset class and dataloader\n",
    "\n",
    "For now we focus on setting up the dataloader, and we will use the provided \n",
    "NRMSdataloader from the ebnerd directory.\n",
    "\n",
    "We will need to\n",
    "- create a mapping from article id to the contrastive vector embedding\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pl.read_parquet(f\"{DATA_PATH}/{df_train_file_name}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles in the mapping: 125541\n",
      "Article ID: 3000022, Encoded Value: shape: (768,)\n",
      "Series: '' [f32]\n",
      "[\n",
      "\t-0.012159\n",
      "\t0.057097\n",
      "\t0.018299\n",
      "\t-0.038884\n",
      "\t-0.010863\n",
      "\t-0.04567\n",
      "\t0.030678\n",
      "\t0.028279\n",
      "\t-0.010959\n",
      "\t0.02433\n",
      "\t0.005196\n",
      "\t0.055502\n",
      "\t…\n",
      "\t-0.034803\n",
      "\t0.001971\n",
      "\t0.000293\n",
      "\t0.016569\n",
      "\t0.0356\n",
      "\t0.056305\n",
      "\t0.026861\n",
      "\t-0.046941\n",
      "\t0.029988\n",
      "\t-0.000547\n",
      "\t-0.037465\n",
      "\t0.025883\n",
      "\t0.013574\n",
      "]\n",
      "Article ID: 3000063, Encoded Value: shape: (768,)\n",
      "Series: '' [f32]\n",
      "[\n",
      "\t0.034482\n",
      "\t0.033533\n",
      "\t0.054598\n",
      "\t-0.023163\n",
      "\t0.009087\n",
      "\t-0.02119\n",
      "\t0.069386\n",
      "\t0.002645\n",
      "\t0.009205\n",
      "\t0.075007\n",
      "\t0.02691\n",
      "\t0.032995\n",
      "\t…\n",
      "\t0.052563\n",
      "\t0.057967\n",
      "\t-0.006074\n",
      "\t-0.002025\n",
      "\t0.026897\n",
      "\t0.049568\n",
      "\t-0.023337\n",
      "\t-0.000465\n",
      "\t0.007559\n",
      "\t-0.035644\n",
      "\t-0.0047\n",
      "\t-0.011909\n",
      "\t-0.023085\n",
      "]\n",
      "Article ID: 3000613, Encoded Value: shape: (768,)\n",
      "Series: '' [f32]\n",
      "[\n",
      "\t-0.014638\n",
      "\t0.030934\n",
      "\t0.036163\n",
      "\t0.039489\n",
      "\t-0.030487\n",
      "\t-0.051596\n",
      "\t-0.025907\n",
      "\t0.042978\n",
      "\t-0.031724\n",
      "\t0.025653\n",
      "\t-0.091189\n",
      "\t-0.002512\n",
      "\t…\n",
      "\t-0.085461\n",
      "\t-0.037983\n",
      "\t0.015554\n",
      "\t-0.003445\n",
      "\t-0.002479\n",
      "\t0.064819\n",
      "\t-0.057671\n",
      "\t-0.072227\n",
      "\t0.034131\n",
      "\t-0.021704\n",
      "\t-0.000715\n",
      "\t-0.021903\n",
      "\t-0.006551\n",
      "]\n",
      "Article ID: 3000700, Encoded Value: shape: (768,)\n",
      "Series: '' [f32]\n",
      "[\n",
      "\t-0.064167\n",
      "\t0.004853\n",
      "\t0.013271\n",
      "\t-0.000364\n",
      "\t0.001436\n",
      "\t-0.04637\n",
      "\t-0.013388\n",
      "\t0.027548\n",
      "\t0.01665\n",
      "\t0.012537\n",
      "\t0.009355\n",
      "\t0.009479\n",
      "\t…\n",
      "\t-0.038878\n",
      "\t0.017438\n",
      "\t0.013741\n",
      "\t0.007699\n",
      "\t0.04404\n",
      "\t-0.009626\n",
      "\t0.035459\n",
      "\t0.047903\n",
      "\t0.027008\n",
      "\t-0.046475\n",
      "\t0.027715\n",
      "\t0.00321\n",
      "\t0.006117\n",
      "]\n",
      "Article ID: 3000840, Encoded Value: shape: (768,)\n",
      "Series: '' [f32]\n",
      "[\n",
      "\t-0.01304\n",
      "\t0.024513\n",
      "\t0.031051\n",
      "\t0.01236\n",
      "\t-0.049199\n",
      "\t-0.027372\n",
      "\t-0.020067\n",
      "\t0.006656\n",
      "\t0.00292\n",
      "\t0.067519\n",
      "\t0.016034\n",
      "\t0.023745\n",
      "\t…\n",
      "\t0.006732\n",
      "\t0.034916\n",
      "\t-0.023499\n",
      "\t0.097725\n",
      "\t-0.005723\n",
      "\t0.035813\n",
      "\t-0.014763\n",
      "\t-0.005555\n",
      "\t0.02879\n",
      "\t-0.032027\n",
      "\t0.012365\n",
      "\t-0.016681\n",
      "\t0.008185\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Creating mapping from article_id to contrastive vector article embeddings\n",
    "article_mapping = create_article_id_to_value_mapping(\n",
    "    df=df_articles, value_col=\"contrastive_vector\", article_col=\"article_id\"\n",
    ")\n",
    "\n",
    "# Print the number of keys in the dictionary\n",
    "print(f\"Number of articles in the mapping: {len(article_mapping)}\")\n",
    "\n",
    "# Print the first few keys and their corresponding values\n",
    "for i, (key, value) in enumerate(article_mapping.items()):\n",
    "  print(f\"Article ID: {key}, Encoded Value: {value}\")\n",
    "  if i >= 4:  # Limit to first 5 entries for brevity\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can define the dataloader for the training\n",
    "train_dataloader = create_dataloader(\n",
    "  df = df_train,\n",
    "  history_column = \"his_article_ids\",\n",
    "  article_dict = article_mapping,\n",
    "  history_size = 30,\n",
    "  embedding_dim = 768, # 768\n",
    "  unknown_representation=\"zeros\",\n",
    "  eval_mode=False,\n",
    "  batch_size=BATCH_SIZE # 32\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0\n",
      "Embeddings: torch.Size([32, 30, 768])\n",
      "Embeddings: torch.Size([32, 5, 768])\n",
      "Labels: torch.Size([32, 5, 1])\n"
     ]
    }
   ],
   "source": [
    "# Print the first batch of the training data loader\n",
    "for i, batch in enumerate(train_dataloader):\n",
    "    X_his, X_pred, y = batch  # batch has two elements: X and y (X is )\n",
    "    print(f\"Batch {i}\")\n",
    "    print(f\"Embeddings: {np.shape(X_his)}\")\n",
    "    print(f\"Embeddings: {np.shape(X_pred)}\")\n",
    "    print(f\"Labels: {np.shape(y)}\")\n",
    "    if i == 0:  # Print only the first batch for brevity\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New shape of X_his: torch.Size([32, 30, 768])\n",
      "New shape of X_pred: torch.Size([160, 1, 768])\n",
      "New shape of y: torch.Size([160, 1])\n"
     ]
    }
   ],
   "source": [
    "# Reshape X_his to 32x30x768\n",
    "X_his = X_his.view(-1, X_his.size(2), X_his.size(3))\n",
    "\n",
    "# Reshape X_pred to 160x1x768\n",
    "X_pred = X_pred.view(-1, 1, X_pred.size(3))\n",
    "\n",
    "# Reshape y to 160x1\n",
    "y = y.view(-1, 1)\n",
    "\n",
    "print(f\"New shape of X_his: {X_his.shape}\")\n",
    "print(f\"New shape of X_pred: {X_pred.shape}\")\n",
    "print(f\"New shape of y: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our dataloader works, and the batches seem reasonable. We should decide on\n",
    "whether we reshape such that the 5 inview articles become 5 distinct training\n",
    "examples, i.e. squeezing together dimensions 32 and 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Implementation of NRMS\n",
    "\n",
    "#### 4.1 Structure\n",
    "\n",
    "**0. Hyperparameters stored in a class**\n",
    "\n",
    "**1. Class:NRMSmodel (based on nn.Module)**\n",
    "- Should take care of building the NewsEncoder, UserEncoder\n",
    "- Should store things such as optimizer and loss\n",
    "- Compute click probability and neg. log likelihood loss\n",
    "- Score click probability by softmax of positive samples\n",
    "- Forward the whole thing right?\n",
    "\n",
    "**2. Class: NewsEncoder**\n",
    "\n",
    "- Should be able to take one article and encode it through mhsa, \n",
    "then add. attention\n",
    "- output should be the encoded article\n",
    "\n",
    "**3. Class: UserEncoder**\n",
    "\n",
    "- Should be able to take all of the articles in history and use mhsa on them\n",
    "as well as add. attention to get user representation\n",
    "\n",
    "\n",
    "**Considerations**\n",
    "- How to work with the dims we have from dataloader?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters for the model\n",
    "\n",
    "class hparams_nrms:\n",
    "    # INPUT DIMENTIONS:\n",
    "    history_size: int = HISTORY_SIZE # 30\n",
    "    embedding_dim: int = 768\n",
    "    batch_size: int = 32\n",
    "    # CONTEXTUAL DIMENSIONS:\n",
    "    npratio : int = NPRATIO\n",
    "    inview_sample_size: int = (NPRATIO + 1)\n",
    "    # MODEL ARCHITECTURE\n",
    "    head_num: int = 16\n",
    "    head_dim: int = 16\n",
    "    attention_hidden_dim: int = 200   # Multi-head self-attention (mhsa)\n",
    "    newsencoder_output_dim: int = 256\n",
    "    additive_attention_dim: int = [512, 512, 512] # second attention layer after mhsa\n",
    "    # MODEL OPTIMIZER:\n",
    "    optimizer : str = \"adam\"\n",
    "    loss: str = \"cross_entropy_loss\"\n",
    "    dropout: float = 0.2\n",
    "    learning_rate: float = 0.001\n",
    "    seed: int = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NRMSmodel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 hparams: dict):\n",
    "        super(NRMSmodel, self).__init__()\n",
    "        self.hparams = hparams\n",
    "\n",
    "        \"\"\"Initialization steps for the NRMS model\"\"\"\n",
    "        # BUILD MODEL\n",
    "        self.model, self.scorer = self._buildNRMS()\n",
    "\n",
    "        # LOSS FUNCTION\n",
    "\n",
    "\n",
    "        # OPTIMIZER\n",
    "        self.optimizer = self._get_opt(\n",
    "            optimizer=self.hparams.optimizer, lr=self.hparams.learning_rate\n",
    "        )\n",
    "\n",
    "    def _buildNRMS(self):\n",
    "\n",
    "        self.newsencoder = NewsEncoder(self.hparams,\n",
    "\n",
    "        return\n",
    "\n",
    "    # def _get_loss(self, loss: str):\n",
    "    #     if loss == \"cross_entropy_loss\":\n",
    "    #         return nn.CrossEntropyLoss()\n",
    "    #     elif loss == \"positive_sample_nll\": # negative log likelihood of positive samples\n",
    "    #         return nn.\n",
    "    #     else:\n",
    "    #         raise ValueError(f\"Loss function {loss} not supported\")\n",
    "\n",
    "    # def click_prob(self, user_representation, news_representation):\n",
    "    #     # Compute dot product and apply softmax\n",
    "    #     pred_one = torch.matmul(news_present_one, user_present.unsqueeze(-1)).squeeze(-1)\n",
    "    #     pred_one = torch.sigmoid(pred_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NRMS(nn.Module):\n",
    "    def __init__(self, hparams, newsencoder):\n",
    "        super(NRMS, self).__init__()\n",
    "        self.hparams = hparams\n",
    "        self.newsencoder = newsencoder\n",
    "        self.userencoder = UserEncoder(hparams, newsencoder)\n",
    "\n",
    "    def forward(self, his_input_title, pred_input_title):\n",
    "\n",
    "        # Encode the predicted titles\n",
    "        batch_size, num_titles, title_size = pred_input_title.size()\n",
    "        pred_input_title = pred_input_title.view(-1, title_size)\n",
    "\n",
    "        # Encode the user history\n",
    "        user_present = self.userencoder(his_input_title)  # u vector\n",
    "        news_encoder = self.newsencoder(pred_input_title)  # r vector\n",
    "\n",
    "        # Apply news_encoder to all inputs in pred_input_title with TimeDistributed\n",
    "        news_present = TimeDistributed(news_encoder, batch_first=False).forward(pred_input_title)\n",
    "\n",
    "        # reshape news_present\n",
    "        news_present = news_present.view(batch_size, num_titles, -1)\n",
    "\n",
    "        # Compute dot product and apply softmax\n",
    "        preds = torch.matmul(news_present, user_present.unsqueeze(-1)).squeeze(-1)\n",
    "        preds = F.softmax(preds, dim=-1)\n",
    "\n",
    "        # Convert to binary labels by taking the argmax\n",
    "        preds = torch.argmax(preds, dim=-1)\n",
    "\n",
    "        return preds\n",
    "\n",
    "    def score(self, his_input_title, pred_input_title_one):\n",
    "        # Encode the user history\n",
    "        user_present = self.userencoder(his_input_title)  # u vector\n",
    "\n",
    "        # Encode the single predicted title\n",
    "        pred_title_one_reshape = pred_input_title_one.view(-1, self.hparams.title_size)\n",
    "        news_present_one = self.newsencoder(pred_title_one_reshape)  # r vector for one article\n",
    "\n",
    "        # Compute dot product and apply sigmoid\n",
    "        pred_one = torch.matmul(news_present_one, user_present.unsqueeze(-1)).squeeze(-1)\n",
    "        pred_one = torch.sigmoid(pred_one)\n",
    "\n",
    "        return pred_one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 hparams,\n",
    "                 units_per_layer=[512, 512, 512]):\n",
    "        super(NewsEncoder, self).__init__()\n",
    "        self.hparams = hparams\n",
    "        self.document_vector_dim = hparams.title_size\n",
    "        self.output_dim = hparams.head_num * hparams.head_dim\n",
    "\n",
    "        self.multihead_attention = nn.MultiheadAttention(embed_dim=self.document_vector_dim, num_heads=hparams.head_num)\n",
    "\n",
    "\n",
    "        layers = []\n",
    "        input_dim = self.document_vector_dim\n",
    "        for units in units_per_layer:\n",
    "            layers.append(nn.Linear(input_dim, units))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.BatchNorm1d(units))\n",
    "            layers.append(nn.Dropout(hparams.dropout))\n",
    "            input_dim = units\n",
    "\n",
    "        layers.append(nn.Linear(input_dim, self.output_dim))\n",
    "        layers.append(nn.ReLU())\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.multihead_attention(x, x, x)\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserEncoder(nn.Module):\n",
    "    def __init__(self, hparams, newsencoder):\n",
    "        super(UserEncoder, self).__init__()\n",
    "        self.newsencoder = TimeDistributed(newsencoder, batch_first=True)\n",
    "        self.newsencoder_output_dim = hparams.newsencoder_output_dim\n",
    "        self.multihead_attention = nn.MultiheadAttention(embed_dim=self.newsencoder_output_dim, num_heads=hparams.head_num)\n",
    "        self.attention_layer = AttentionLayer2(hparams)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encode the news history\n",
    "        encoded_news = self.newsencoder(x)\n",
    "\n",
    "        # Apply multi-head attention\n",
    "        attn_output, _ = self.multihead_attention(encoded_news, encoded_news, encoded_news)\n",
    "\n",
    "        # Apply the attention layer\n",
    "        user_representation = self.attention_layer(attn_output)\n",
    "\n",
    "        return user_representation\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLProject_recsys-ebnerd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
